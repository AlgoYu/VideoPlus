#include <jni.h>
#include <android/log.h>

extern "C" {
#include	"libavcodec/avcodec.h"
#include	"libavformat/avformat.h"
#include	"libavutil/opt.h"
#include	"libavutil/imgutils.h"
#include	"libswscale/swscale.h"
}

#include "BasicUsageEnvironment.hh"
#include "GroupsockHelper.hh"
#include "liveMedia.hh"
#include "com_xproject_streamer_Streamer.h"

#define LOG_TAG "Streamer"
#define LOGD(...)  __android_log_print(ANDROID_LOG_DEBUG, LOG_TAG, __VA_ARGS__)
#define LOGI(...)  __android_log_print(ANDROID_LOG_INFO, LOG_TAG, __VA_ARGS__)
#define LOGE(...)  __android_log_print(ANDROID_LOG_ERROR, LOG_TAG, __VA_ARGS__)

#define IMPLEMENT_RTSP_SERVER
//#define USE_SSM 1
#ifdef USE_SSM
Boolean const isSSM = True;//采用多播
#else
Boolean const isSSM = False;
#endif

#define TRANSPORT_PACKET_SIZE 188
#define TRANSPORT_PACKETS_PER_NETWORK_PACKET 7


void play();
void afterPlaying(void* /*clientData*/);

AVCodecContext *codecContext;
FILE *file;
AVFrame *frame;
AVFrame *tmpFrame;
AVPacket packet;
int count, got_output;
SwsContext *swsContext;

UsageEnvironment* uEnv;
H264VideoStreamFramer* videoSource;
RTPSink* videoSink;

char const *inputFilename;

JNIEXPORT void JNICALL Java_com_xproject_streamer_Streamer_init(JNIEnv *env,
		jobject obj, jstring filename, jint width, jint height,
		jint frameRate) {
	LOGD("init()");

	av_register_all();

	AVCodec *codec = avcodec_find_encoder(AV_CODEC_ID_H264);
	if (!codec) {
		LOGE("AV_CODEC_ID_H264 codec not found!");
		exit(1);
	}

	codecContext = avcodec_alloc_context3(codec);
	if (!codec) {
		LOGE("couldn't allocate codec context");
		exit(1);
	}

	/* put sample parameters */
	codecContext->bit_rate = 400000;
	/* resolution must be a multiple of two */
	codecContext->width = width;
	codecContext->height = height;
	/* frames per second */
	codecContext->time_base = (AVRational ) {1, frameRate};
	codecContext->gop_size = frameRate; /* emit one intra frame every ten frames */
	codecContext->max_b_frames = 1;
	codecContext->pix_fmt = AV_PIX_FMT_YUV420P;

	av_opt_set(codecContext->priv_data, "profile", "baseline", 0);
	av_opt_set(codecContext->priv_data, "preset", "ultrafast", 0);

	if (avcodec_open2(codecContext, codec, NULL) < 0) {
		LOGE("couldn't open codec");
		exit(1);
	}

	inputFilename = env->GetStringUTFChars(filename, NULL);
	file = fopen(inputFilename, "wb");
	if (!file) {
		LOGE("couldn't open %s", inputFilename);
		exit(1);
	}

	frame = avcodec_alloc_frame();
	if (!frame) {
		LOGE("couldn't allocate frame");
		exit(1);
	}

	frame->format = codecContext->pix_fmt;
	frame->width = codecContext->width;
	frame->height = codecContext->height;

	if (av_image_alloc(frame->data, frame->linesize, codecContext->width,
			codecContext->height, codecContext->pix_fmt, 32) < 0) {
		LOGE("couldn't allocate raw picture buffer");
		exit(1);
	}

	tmpFrame = avcodec_alloc_frame();
	if (!tmpFrame) {
		LOGE("couldn't allocate frame");
		exit(1);
	}

	if (av_image_alloc(tmpFrame->data, tmpFrame->linesize, codecContext->width,
			codecContext->height, AV_PIX_FMT_NV21, 32) < 0) {
		LOGE("couldn't allocate raw picture buffer");
		exit(1);
	}

	count = 0;
}

JNIEXPORT void JNICALL Java_com_xproject_streamer_Streamer_encode(JNIEnv *env,
		jobject obj, jbyteArray data) {
	av_init_packet(&packet);
	packet.data = NULL;
	packet.size = 0;

	swsContext = sws_getCachedContext(swsContext, codecContext->width,
			codecContext->height, AV_PIX_FMT_NV21, codecContext->width,
			codecContext->height, codecContext->pix_fmt, SWS_BILINEAR, NULL,
			NULL, NULL);

	jbyte *_data = env->GetByteArrayElements(data, NULL);

	avpicture_fill((AVPicture*) tmpFrame, (const unsigned char*) _data,
			AV_PIX_FMT_NV21, codecContext->width, codecContext->height);

	env->ReleaseByteArrayElements(data, _data, 0);

	sws_scale(swsContext, tmpFrame->data, tmpFrame->linesize, 0,
			codecContext->height, frame->data, frame->linesize);

	frame->pts = count;

	/* encode the image */
	if (avcodec_encode_video2(codecContext, &packet, frame, &got_output)) {
		LOGE("couldn't encode frame");
		exit(1);
	}

	if (got_output) {
//		LOGI("write frame %3d (size=%5d)", count, packet.size);
		fwrite(packet.data, 1, packet.size, file);
		av_free_packet(&packet);
	}

	count++;
}

JNIEXPORT void JNICALL Java_com_xproject_streamer_Streamer_deinit(JNIEnv *env,
		jobject obj) {
	LOGD("deinit()");

	/* get the delayed frames */
	for (got_output = 1; got_output; count++) {
		if (avcodec_encode_video2(codecContext, &packet, NULL, &got_output) < 0) {
			LOGE("couldn't encode frame");
			exit(1);
		}

		if (got_output) {
//			LOGI("write frame %3d (size=%5d)", count, packet.size);
			fwrite(packet.data, 1, packet.size, file);
			av_free_packet(&packet);
		}
	}

	uint8_t endcode[] = { 0, 0, 1, 0xb7 };

	/* add sequence end code to have a real mpeg file */
	fwrite(endcode, 1, sizeof(endcode), file);
	fclose(file);

	avcodec_close(codecContext);
	av_free(codecContext);
	av_freep(&frame->data[0]);
	avcodec_free_frame(&frame);
	avcodec_free_frame(&tmpFrame);
}

#if 0
JNIEXPORT void JNICALL Java_com_xproject_streamer_Streamer_loop(JNIEnv *env,
		jobject obj, jstring addr) {
	// Begin by setting up our usage environment:
	TaskScheduler* scheduler = BasicTaskScheduler::createNew();
	uEnv = BasicUsageEnvironment::createNew(*scheduler);

	// Create 'groupsocks' for RTP and RTCP:
	struct in_addr destinationAddress;
	const char *_addr = env->GetStringUTFChars(addr, NULL);
	destinationAddress.s_addr = our_inet_addr(_addr); /*chooseRandomIPv4SSMAddress(*uEnv);*/
	env->ReleaseStringUTFChars(addr, _addr);
	// Note: This is a multicast address.  If you wish instead to stream
	// using unicast, then you should use the "testOnDemandRTSPServer"
	// test program - not this test program - as a model.

	const unsigned short rtpPortNum = 18888;
	const unsigned short rtcpPortNum = rtpPortNum + 1;
	const unsigned char ttl = 255;

	const Port rtpPort(rtpPortNum);
	const Port rtcpPort(rtcpPortNum);

	Groupsock rtpGroupsock(*uEnv, destinationAddress, rtpPort, ttl);
	Groupsock rtcpGroupsock(*uEnv, destinationAddress, rtcpPort, ttl);

	// Create a 'H264 Video RTP' sink from the RTP 'groupsock':
	OutPacketBuffer::maxSize = 100000;
	videoSink = H264VideoRTPSink::createNew(*uEnv, &rtpGroupsock, 96);

	// Create (and start) a 'RTCP instance' for this RTP sink:
	const unsigned estimatedSessionBandwidth = 500; // in kbps; for RTCP b/w share
	const unsigned maxCNAMElen = 100;
	unsigned char CNAME[maxCNAMElen + 1];
	gethostname((char*) CNAME, maxCNAMElen);
	CNAME[maxCNAMElen] = '\0'; // just in case

	RTCPInstance* rtcp = RTCPInstance::createNew(*uEnv, &rtcpGroupsock,
			estimatedSessionBandwidth, CNAME, videoSink, NULL /* we're a server */, True /* we're a SSM source */);

	// Note: This starts RTCP running automatically
	RTSPServer* rtspServer = RTSPServer::createNew(*uEnv, 8554);
	if (rtspServer == NULL) {
		LOGE("Failed to create RTSP server: %s", uEnv->getResultMsg());
		exit(1);
	}

	ServerMediaSession* sms = ServerMediaSession::createNew(*uEnv, "streamer",
			inputFilename, "Session streamed by \"Streamer\"", True /*SSM*/);
	sms->addSubsession(PassiveServerMediaSubsession::createNew(*videoSink, rtcp));
	rtspServer->addServerMediaSession(sms);

	char* url = rtspServer->rtspURL(sms);
	LOGI("Play this Stream using the URL: \"%s\"", url);
	delete[] url;

	// Start the streaming:
	LOGI("Now Beginning streaming\n");
	play();

	uEnv->taskScheduler().doEventLoop(); // does not return
}

void play() {
	// Open the input file as a 'byte-stream file source':
	ByteStreamFileSource *fileSource = ByteStreamFileSource::createNew(*uEnv, inputFilename);
	if (fileSource == NULL) {
		LOGE("Unable to open file \"%s\" as a byte-stream file source", inputFilename);
		exit(1);
	}

	FramedSource* videoES = fileSource;

	// Create a framer for the Video Elementary Stream:
	videoSource = H264VideoStreamFramer::createNew(*uEnv, videoES);

	// Finally, start playing:
	LOGI("Beginning to read from file...\n");
	videoSink->startPlaying(*videoSource, afterPlaying, videoSink);
}

void afterPlaying(void* /*clientData*/) {
	videoSink->stopPlaying();
	Medium::close(videoSource);
	// Note that this also closes the input file that this source read from.

	// Start playing once again:
	play();
}

#else
JNIEXPORT void JNICALL Java_com_xproject_streamer_Streamer_loop(JNIEnv *env,
		jobject obj, jstring addr) {

  // 首先建立使用环境：
  TaskScheduler* scheduler = BasicTaskScheduler::createNew();
  uEnv = BasicUsageEnvironment::createNew(*scheduler);
 
  // 创建 'groupsocks' for RTP and RTCP:
  char const* destinationAddressStr
#ifdef USE_SSM
    = "232.255.42.42";
#else
    = "239.255.42.42";
  // Note: 这是一个多播地址。如果你希望流使用单播地址,然后替换这个字符串与单播地址  
#endif
  const unsigned short rtpPortNum = 18888;
  const unsigned short rtcpPortNum = rtpPortNum+1;
  const unsigned char ttl = 7; //
 
  // Create 'groupsocks' for RTP and RTCP:
  struct in_addr destinationAddress;

#ifndef USE_SSM
  const char *_addr = env->GetStringUTFChars(addr, NULL);
  destinationAddress.s_addr = our_inet_addr(_addr); /*chooseRandomIPv4SSMAddress(*uEnv);*/
  env->ReleaseStringUTFChars(addr, _addr);
#else
  destinationAddress.s_addr = our_inet_addr(destinationAddressStr);
#endif

  const Port rtpPort(rtpPortNum);
  const Port rtcpPort(rtcpPortNum);
 
  Groupsock rtpGroupsock(*uEnv, destinationAddress, rtpPort, ttl);
  Groupsock rtcpGroupsock(*uEnv, destinationAddress, rtcpPort, ttl);
  
#ifdef USE_SSM
  rtpGroupsock.multicastSendOnly();
  rtcpGroupsock.multicastSendOnly();
#endif
 
  // 创建一个适当的“RTPSink”:
  videoSink = H264VideoRTPSink::createNew(*uEnv, &rtpGroupsock, 96);
 
  const unsigned estimatedSessionBandwidth = 5000; // in kbps; for RTCP b/w share
  const unsigned maxCNAMElen = 100;
  unsigned char CNAME[maxCNAMElen+1];
  gethostname((char*)CNAME, maxCNAMElen);
  CNAME[maxCNAMElen] = '\0';
  
#ifdef IMPLEMENT_RTSP_SERVER
  RTCPInstance* rtcp =
#endif
  RTCPInstance::createNew(*uEnv, &rtcpGroupsock,
                estimatedSessionBandwidth, CNAME,
                videoSink, NULL /* we're a server */, isSSM);

  // 开始自动运行的媒体
#ifdef IMPLEMENT_RTSP_SERVER
  RTSPServer* rtspServer = RTSPServer::createNew(*uEnv, 8554);
  if (rtspServer == NULL) {
    LOGE("Failed to create RTSP server: %s ", uEnv->getResultMsg());
    exit(1);
  }
  
  ServerMediaSession* sms
    = ServerMediaSession::createNew(*uEnv, "Streamer", inputFilename,
           "Session streamed by \"Streamer\"", isSSM);
  sms->addSubsession(PassiveServerMediaSubsession::createNew(*videoSink, rtcp));
  rtspServer->addServerMediaSession(sms);
 
  char* url = rtspServer->rtspURL(sms);
  LOGI("Play this stream using the URL %s", url);
  
  delete[] url;
#endif
  
  LOGD("开始发送流媒体...");
  play();
 
  uEnv->taskScheduler().doEventLoop(); 
}
 
void afterPlaying(void* /*clientData*/) {
  LOGD("...从文件中读取完毕");
 
  Medium::close(videoSource);
  // 将关闭从源读取的输入文件
 
  play();
}
 
void play() {
  unsigned const inputDataChunkSize = TRANSPORT_PACKETS_PER_NETWORK_PACKET*TRANSPORT_PACKET_SIZE;
 
  // 打开输入文件作为一个“ByteStreamFileSource":
  ByteStreamFileSource* fileSource
    = ByteStreamFileSource::createNew(*uEnv, inputFilename, inputDataChunkSize);
  if (fileSource == NULL) {
    LOGE("无法打开文件%s作为 file source", inputFilename);
    exit(1);
  }
   
  videoSource = H264VideoStreamFramer::createNew(*uEnv, fileSource);
   
  LOGD("Beginning to read from file...");
  videoSink->startPlaying(*videoSource, afterPlaying, videoSink);
}


#endif

